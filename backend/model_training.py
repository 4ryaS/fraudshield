# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18JDdzoHCW_rcjk_OLqWwy4HFv5FDkKKg
"""

# CHUNK 1: IMPORTS AND DATA LOADING
# Import required libraries
import pandas as pd
import numpy as np
import joblib
from google.colab import drive
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import IsolationForest
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
import lightgbm as lgb
from sklearn.metrics import accuracy_score, classification_report

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)

# Path to CSV file in Google Drive
file_path = "/content/drive/MyDrive/Fraud.csv"

# Read the CSV file into a Pandas DataFrame
df = pd.read_csv(file_path)

# Display basic information about the data
print("First few rows of the dataset:")
print(df.head())

# Check for missing values
print("\nMissing values in each column:")
print(df.isnull().sum())

# CHUNK 2: DATA PREPROCESSING
# Data preprocessing
# DO NOT drop nameOrig and nameDest as they're needed for behavioral analysis
# Create a copy of the original dataframe for behavioral analysis
behavioral_df = df.copy()

# For transaction pattern analysis and anomaly detection
transaction_df = df.copy()
# We'll drop identity columns only for non-behavioral models
transaction_df.drop(['nameOrig', 'nameDest'], axis=1, inplace=True)

# One-hot encode categorical variables
transaction_df = pd.get_dummies(transaction_df, columns=['type'], drop_first=True)
behavioral_df = pd.get_dummies(behavioral_df, columns=['type'], drop_first=True)

# Feature engineering for transaction analysis
transaction_df['balance_difference'] = transaction_df['oldbalanceOrg'] - transaction_df['newbalanceOrig']
transaction_df['dest_balance_difference'] = transaction_df['oldbalanceDest'] - transaction_df['newbalanceDest']
transaction_df['large_transaction'] = (transaction_df['amount'] > transaction_df['amount'].quantile(0.95)).astype(int)

# Feature engineering for behavioral analysis
behavioral_df['balance_difference'] = behavioral_df['oldbalanceOrg'] - behavioral_df['newbalanceOrig']
behavioral_df['dest_balance_difference'] = behavioral_df['oldbalanceDest'] - behavioral_df['newbalanceDest']
behavioral_df['large_transaction'] = (behavioral_df['amount'] > behavioral_df['amount'].quantile(0.95)).astype(int)

# CHUNK 3: TRANSACTION MODEL PREPARATION
# Select features for transaction-based models
transaction_features = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest',
                       'balance_difference', 'dest_balance_difference', 'large_transaction']
if 'type_CASH_OUT' in transaction_df.columns:
    transaction_features.extend([col for col in transaction_df.columns if col.startswith('type_')])

# Define X and y for transaction analysis
X_transaction = transaction_df[transaction_features]
y_transaction = transaction_df['isFraud']

# Handle class imbalance for transaction models
smote = SMOTE(sampling_strategy=0.3, random_state=42)
X_transaction_resampled, y_transaction_resampled = smote.fit_resample(X_transaction, y_transaction)

# Split transaction data
X_transaction_train, X_transaction_test, y_transaction_train, y_transaction_test = train_test_split(
    X_transaction_resampled, y_transaction_resampled, test_size=0.3, random_state=42)

print("Transaction training set size:", X_transaction_train.shape)
print("Transaction testing set size:", X_transaction_test.shape)

# Standardize features for transaction models
numeric_features = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest',
                   'balance_difference', 'dest_balance_difference']
transaction_scaler = StandardScaler()
X_transaction_train[numeric_features] = transaction_scaler.fit_transform(X_transaction_train[numeric_features])
X_transaction_test[numeric_features] = transaction_scaler.transform(X_transaction_test[numeric_features])

# Save the transaction scaler
joblib.dump(transaction_scaler, "transaction_scaler.pth")

# CHUNK 4: ISOLATION FOREST MODEL
# Train Isolation Forest on transaction features only
iso_forest_features = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']
X_iso = X_transaction_train[iso_forest_features]

iso_forest = IsolationForest(contamination=0.02, random_state=42)
iso_forest.fit(X_iso)

# Save the model
joblib.dump(iso_forest, "isolation_forest.pth")

# CHUNK 5: XGBOOST TRANSACTION MODEL
# Train XGBoost on all transaction features
transaction_xgb = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    n_jobs=-1,
    random_state=42
)
transaction_xgb.fit(X_transaction_train, y_transaction_train)

# Save the model
joblib.dump(transaction_xgb, "transaction_monitoring_agent.pth")

# CHUNK 6: BEHAVIORAL ANALYSIS DATA PREPARATION (OPTIMIZED)
print("\nPreparing data for behavioral analysis...")

# More efficient approach using pandas groupby
# First, create a subset of needed columns to reduce memory usage
columns_needed = ['nameOrig', 'amount', 'oldbalanceOrg', 'balance_difference', 'large_transaction', 'isFraud']
columns_needed.extend([col for col in behavioral_df.columns if col.startswith('type_')])
behavioral_subset = behavioral_df[columns_needed]

# Group the data and calculate aggregated statistics
behavior_features = behavioral_subset.groupby('nameOrig').agg(
    avg_transaction_amount=('amount', 'mean'),
    max_transaction_amount=('amount', 'max'),
    transaction_amount_std=('amount', 'std'),
    avg_balance=('oldbalanceOrg', 'mean'),
    transaction_count=('amount', 'count'),
    large_transaction_ratio=('large_transaction', 'mean'),
    balance_change_mean=('balance_difference', 'mean'),
    is_fraud=('isFraud', lambda x: 1 if x.sum() > 0 else 0)
)

# Add transaction type ratios
for type_col in [col for col in behavioral_subset.columns if col.startswith('type_')]:
    behavior_features[f'{type_col}_ratio'] = behavioral_subset.groupby('nameOrig')[type_col].mean()

# Reset index to convert nameOrig from index to column
behavior_features_df = behavior_features.reset_index()

# Handle NaN values from std calculation when there's only one transaction
behavior_features_df = behavior_features_df.fillna(0)

# Store user_ids and drop from features
user_ids = behavior_features_df['nameOrig']
behavior_features_df = behavior_features_df.drop('nameOrig', axis=1)

print(f"Created behavioral features for {len(behavior_features_df)} users")

# CHUNK 7: BEHAVIORAL MODEL PREPARATION
# Split behavioral data
X_behavior = behavior_features_df.drop('is_fraud', axis=1)
y_behavior = behavior_features_df['is_fraud']

# Handle class imbalance for behavioral models if needed
if sum(y_behavior) / len(y_behavior) < 0.1:  # If fraud ratio is less than 10%
    behavior_smote = SMOTE(sampling_strategy=0.3, random_state=42)
    X_behavior_resampled, y_behavior_resampled = behavior_smote.fit_resample(X_behavior, y_behavior)
else:
    X_behavior_resampled, y_behavior_resampled = X_behavior, y_behavior

# Split behavior data
X_behavior_train, X_behavior_test, y_behavior_train, y_behavior_test = train_test_split(
    X_behavior_resampled, y_behavior_resampled, test_size=0.3, random_state=42)

print("Behavioral training set size:", X_behavior_train.shape)
print("Behavioral testing set size:", X_behavior_test.shape)

# Standardize behavioral features
behavior_scaler = StandardScaler()
X_behavior_train = behavior_scaler.fit_transform(X_behavior_train)
X_behavior_test = behavior_scaler.transform(X_behavior_test)

# Save the behavioral scaler
joblib.dump(behavior_scaler, "behavior_scaler.pth")

# CHUNK 8: BEHAVIORAL MODEL TRAINING
# Train Logistic Regression on behavioral features
behavior_model = LogisticRegression(max_iter=1000, C=1.0, solver='liblinear', random_state=42)
behavior_model.fit(X_behavior_train, y_behavior_train)

# Save model
joblib.dump(behavior_model, "behavioral_analysis_agent.pth")

# CHUNK 9: RISK SCORING MODEL
# Train LightGBM model for risk scoring based on transaction features
risk_agent = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.05, num_leaves=31, n_jobs=-1)
risk_agent.fit(X_transaction_train, y_transaction_train)

# Save model
joblib.dump(risk_agent, "risk_scoring_agent.pth")

# CHUNK 10: MODEL EVALUATION
print("\nEvaluating Transaction Models:")
# Predict on test data using transaction models
y_pred_transaction = transaction_xgb.predict(X_transaction_test)
y_pred_risk = risk_agent.predict(X_transaction_test)

# Compute accuracy for transaction models
acc_transaction = accuracy_score(y_transaction_test, y_pred_transaction)
acc_risk = accuracy_score(y_transaction_test, y_pred_risk)

print("XGBoost Transaction Model Accuracy:", acc_transaction)
print("LightGBM Risk Model Accuracy:", acc_risk)
print("\nXGBoost Classification Report:")
print(classification_report(y_transaction_test, y_pred_transaction))

print("\nEvaluating Behavioral Model:")
# Predict on test data using behavioral model
y_pred_behavior = behavior_model.predict(X_behavior_test)

# Compute accuracy for behavioral model
acc_behavior = accuracy_score(y_behavior_test, y_pred_behavior)

print("Logistic Regression Behavioral Model Accuracy:", acc_behavior)
print("\nBehavioral Model Classification Report:")
print(classification_report(y_behavior_test, y_pred_behavior))